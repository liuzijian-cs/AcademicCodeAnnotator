# cite: https://github.com/SmartFlowAI/Llama3-XTuner-CN/blob/main/web_demo.py
# cite: https://github.com/CrazyBoyM/llama3-Chinese-chat

import copy
import warnings
import streamlit as st
import torch
from torch import nn
from dataclasses import asdict, dataclass
from typing import Callable, List, Optional
from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList
from transformers.utils import logging
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # isort: skip
from peft import PeftModel
from utils.base_function import Color

"""
:copy: ä¾›äº†ç”¨äºæµ…å¤åˆ¶å’Œæ·±å¤åˆ¶å¯å˜å¯¹è±¡çš„åŠŸèƒ½, ä»£ç ä¸­ä½¿ç”¨äº†copy.deepcopy() ç”¨äºåˆ›å»ºä¸€ä¸ªå¯¹è±¡åŠå…¶åŒ…å«çš„å¯¹è±¡çš„å®Œæ•´æ‹·è´ã€‚
:warnings: ç”¨äºå‘å‡ºè­¦å‘Šï¼Œä»¥æç¤ºç¨‹åºçš„æŸäº›é—®é¢˜ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ã€‚
:streamlit: ç”¨äºå¿«é€Ÿåˆ›å»ºå’Œå…±äº«æ•°æ®åº”ç”¨çš„Pythonåº“ã€‚
:torch: æ˜¯PyTorchçš„æ ¸å¿ƒåº“ï¼Œæ˜¯ä¸€ä¸ªç”¨äºæ·±åº¦å­¦ä¹ çš„åº“ï¼Œæ”¯æŒå¼ºå¤§çš„å¼ é‡è®¡ç®—ï¼ˆç±»ä¼¼äºNumPyï¼‰ä»¥åŠè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ç­‰åŠŸèƒ½ã€‚
:torch.nn: è¿™æ˜¯PyTorchä¸­çš„å­æ¨¡å—ï¼Œæä¾›äº†è®¸å¤šå»ºç«‹å’Œè®­ç»ƒç¥ç»ç½‘ç»œæ‰€éœ€çš„å·¥å…·å’Œå±‚ã€‚
:dataclasses: ç”¨äºè‡ªåŠ¨æ·»åŠ ç‰¹æ®Šæ–¹æ³•ï¼ˆå¦‚__init__() å’Œ __repr__()ï¼‰åˆ°ç±»ä¸­ï¼Œä¸»è¦ç”¨äºå¿«é€Ÿåˆ›å»ºä¸»è¦ç”¨äºå­˜å‚¨æ•°æ®çš„ç±»ã€‚
    asdict() å‡½æ•°å°†æ•°æ®ç±»å®ä¾‹è½¬æ¢ä¸ºå­—å…¸ï¼Œä¾¿äºå¤„ç†ã€‚
:typing: è¿™ä¸ªæ¨¡å—æ”¯æŒPythonçš„ç±»å‹æç¤ºï¼Œç”¨äºåœ¨ä»£ç ä¸­æ·»åŠ å˜é‡çš„ç±»å‹ä¿¡æ¯ï¼Œæé«˜ä»£ç çš„å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚
    Callable, List, Optional æ˜¯å¸¸ç”¨çš„ç±»å‹ï¼Œåˆ†åˆ«ç”¨äºå‡½æ•°ç±»å‹ã€åˆ—è¡¨ç±»å‹å’Œå¯é€‰ç±»å‹çš„æ³¨è§£ã€‚
:transformers.generation.utils: æä¾›äº†ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„å·¥å…·ï¼Œå¦‚LogitsProcessorListå’ŒStoppingCriteriaListã€‚
    è¿™äº›ç±»ç”¨äºæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œä¾‹å¦‚ä¿®æ”¹logitsæ¥å¼•å¯¼ç”Ÿæˆçš„å†…å®¹ï¼Œä»¥åŠå®šä¹‰ä½•æ—¶åœæ­¢æ–‡æœ¬ç”Ÿæˆã€‚
:transformers.utils:transformersåº“çš„ä¸€ä¸ªå·¥å…·æ¨¡å—ï¼ŒåŒ…å«æ—¥å¿—å’Œå…¶ä»–è¾…åŠ©åŠŸèƒ½ã€‚
    å…¶ä¸­logging.get_logger()ç”¨äºè·å–æ—¥å¿—å¯¹è±¡ï¼Œä»¥è®°å½•ç¨‹åºè¿è¡Œæ—¶çš„å„ç§ä¿¡æ¯ã€‚
:transformers: AutoTokenizer å’Œ AutoModelForCausalLM æ˜¯transformersåº“çš„ç»„ä»¶ï¼Œç”¨äºè‡ªåŠ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œç›¸åº”çš„åˆ†è¯å™¨ã€‚
    è¿™äº›å·¥å…·ä¸»è¦ç”¨äºNLPä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆæˆ–è¯­è¨€æ¨¡å‹è¯„ä¼°ã€‚
:peft: PeftModel å¾ˆå¯èƒ½æ˜¯ä¸€ä¸ªä¸“é—¨çš„æ¨¡å‹æˆ–å·¥å…·ï¼Œç”¨äºåŠ è½½å’Œè¿è¡Œç‰¹å®šçš„é¢„è®­ç»ƒæ¨¡å‹æˆ–é€‚é…å™¨ã€‚è¿™å¯èƒ½æ˜¯ç‰¹å®šäºæŸä¸ªé¡¹ç›®æˆ–åº“çš„è‡ªå®šä¹‰å®ç°ã€‚
:utils.base_function: æœ¬ä»£ç è‡ªå®šä¹‰çš„åŸºç¡€åŠŸèƒ½
    Color å®šä¹‰äº†å¸¸è§çš„ASCIIé¢œè‰²è½¬ä¹‰ç¬¦
"""

logger = logging.get_logger(__name__)  # åˆå§‹åŒ–æ—¥å¿—è®¾ç½®

# æ­¤å‡½æ•°å¯ç”¨äºè®¾ç½®å…¶ä»–é¡µé¢å±æ€§ï¼Œå¦‚å¸ƒå±€å’Œåˆå§‹ä¾§è¾¹æ çŠ¶æ€ï¼Œä½†åœ¨æ­¤ä»£ç æ®µä¸­ä»…è®¾ç½®äº†é¡µé¢æ ‡é¢˜ã€‚
st.set_page_config(page_title="Academic Code Annotator (with LLAMA3 ğŸ˜Š)")  # Streamlit é¡µé¢é…ç½®:è®¾ç½®äº†é¡µé¢çš„æ ‡é¢˜


@dataclass
class GenerationConfig:
    """
    æ­¤é…ç½®ç”¨äºèŠå¤©ï¼Œä»¥æä¾›å¯¹è¯çš„å¤šæ ·æ€§
    """
    max_length: int = 65535  # å®šä¹‰ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ä¸º 65535 ä¸ªå­—ç¬¦ã€‚è¿™æ˜¯ç”Ÿæˆè¿‡ç¨‹ä¸­å¯ä»¥ç”Ÿæˆçš„å­—ç¬¦çš„ç»å¯¹ä¸Šé™ã€‚
    # max_new_tokens: int = 600  # è®¾ç½®ç”Ÿæˆè°ƒç”¨ä¸­æ–°ç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•°ï¼ˆä¾‹å¦‚ï¼Œå•è¯æˆ–æ ‡ç‚¹ç¬¦å·ï¼‰ã€‚è¿™æœ‰åŠ©äºæ§åˆ¶è¾“å‡ºæ–‡æœ¬çš„å¤§å°ã€‚
    top_p: float = 0.8  # ç”Ÿæˆæ–‡æœ¬æ—¶çš„éšæœºé‡‡æ ·ç­–ç•¥ã€‚top_p ä¸º 0.8 è¡¨ç¤ºåœ¨æ¯ä¸€æ­¥ï¼Œåªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡è´¨é‡è‡³å°‘å æ€»æ¦‚ç‡è´¨é‡ 80% çš„æœ€é«˜æ¦‚ç‡çš„è¯æ±‡ã€‚
    temperature: float = 0.8  # æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹çš„éšæœºæ€§ã€‚æ¸©åº¦è¶Šä½ï¼Œè¾“å‡ºè¶Šå€¾å‘äºé«˜æ¦‚ç‡é€‰é¡¹ã€‚0.8 æ˜¯ä¸€ä¸ªä½¿è¾“å‡ºæ—¢éšæœºåˆå¯é çš„ä¸­é—´å€¼ã€‚
    do_sample: bool = True  # æ˜¯å¦åœ¨ç”Ÿæˆæ—¶ä½¿ç”¨é‡‡æ ·ç­–ç•¥ã€‚è®¾ç½®ä¸º True è¡¨ç¤ºå¯ç”¨é‡‡æ ·ï¼Œè¿™é€šå¸¸ä¸ top_p æˆ– temperature ç»“åˆä»¥å¢åŠ è¾“å‡ºçš„å¤šæ ·æ€§ã€‚
    repetition_penalty: float = 1.05  # é‡å¤æƒ©ç½šï¼Œè¯¥å‚æ•°ç”¨æ¥é™ä½é‡å¤è¯æ±‡çš„å‡ºç°æ¦‚ç‡ã€‚1.05 è¡¨ç¤ºå¯¹äºé‡å¤çš„è¯ï¼Œå…¶é€‰æ‹©æ¦‚ç‡ä¼šè¢«ç•¥å¾®é™ä½ã€‚


@torch.inference_mode()  # è£…é¥°å™¨ç”¨äºä¼˜åŒ–æ€§èƒ½ï¼Œåœ¨æ­¤æ¨¡å¼ä¸‹ï¼ŒPyTorch å°†ä¸ä¼šè®¡ç®—æ¢¯åº¦ï¼Œè¿™å¯¹äºæ¨ç†ç‰¹åˆ«æœ‰ç”¨ã€‚
def generate_interactive(
        model,
        tokenizer,
        prompt,
        generation_config: Optional[GenerationConfig] = None,
        logits_processor: Optional[LogitsProcessorList] = None,
        stopping_criteria: Optional[StoppingCriteriaList] = None,
        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor],
        List[int]]] = None,
        additional_eos_token_id: Optional[int] = None,
        **kwargs,  #
):
    """
    :param model: é¢„è®­ç»ƒæ¨¡å‹
    :param tokenizer: ä¸æ¨¡å‹ç›¸é…çš„åˆ†è¯å™¨ï¼Œç”¨äºæ–‡æœ¬çš„ç¼–ç å’Œè§£ç ã€‚
    :param prompt: åˆå§‹æ–‡æœ¬æç¤ºï¼Œç”¨äºå¯åŠ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ã€‚
    :param generation_config: ä¸€ä¸ªå¯é€‰çš„ GenerationConfig æ•°æ®ç±»å®ä¾‹ï¼ŒåŒ…å«ç”Ÿæˆæ–‡æœ¬æ—¶çš„é…ç½®ã€‚
    :param logits_processor: ç”¨äºè°ƒæ•´ç”Ÿæˆçš„ logitsï¼Œå¯ä»¥å®ç°è‡ªå®šä¹‰çš„ç”Ÿæˆç­–ç•¥ã€‚
    :param stopping_criteria: å®šä¹‰ä½•æ—¶åœæ­¢ç”Ÿæˆæ–‡æœ¬çš„æ¡ä»¶ã€‚
    :param prefix_allowed_tokens_fn: ä¸€ä¸ªå‡½æ•°ï¼Œå®šä¹‰å“ªäº›tokenå¯ä»¥åœ¨ç‰¹å®šä½ç½®è¢«ç”Ÿæˆã€‚
    :param additional_eos_token_id: é¢å¤–çš„ç»“æŸç¬¦token IDï¼Œç”¨äºæ‰©å±•åœæ­¢ç”Ÿæˆçš„æ¡ä»¶ã€‚
    :param kwargs: å…¶ä»–ä¼ é€’ç»™æ¨¡å‹çš„å…³é”®å­—å‚æ•°ã€‚
    """
    # å°†æ–‡æœ¬æç¤ºï¼ˆpromptï¼‰è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ï¼ˆé€šå¸¸æ˜¯token IDsï¼‰ã€‚return_tensors='pt' æŒ‡ç¤ºåˆ†è¯å™¨è¿”å› PyTorch å¼ é‡ã€‚
    inputs = tokenizer([prompt], return_tensors='pt')
    # è®¡ç®—è¾“å…¥çš„tokenæ•°é‡ï¼ˆå³è¾“å…¥æ–‡æœ¬çš„é•¿åº¦ï¼‰ã€‚
    input_length = len(inputs['input_ids'][0])
    # å¾ªç¯éå†æ‰€æœ‰è¾“å…¥å¼ é‡ï¼Œå°†å®ƒä»¬ç§»åŠ¨åˆ° GPU ä¸Šï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥åœ¨ GPU ä¸Šè¿›è¡Œè®¡ç®—ã€‚
    for k, v in inputs.items():
        inputs[k] = v.cuda()
    # ä» inputs å­—å…¸ä¸­æå– input_ids å¼ é‡ï¼Œè¿™æ˜¯åç»­ç”Ÿæˆè¿‡ç¨‹çš„ä¸»è¾“å…¥ã€‚
    input_ids = inputs['input_ids']
    # è¿™è¡Œä»£ç æå– input_ids å¼ é‡çš„ç»´åº¦ï¼Œå…¶ä¸­ input_ids_seq_length æ˜¯åºåˆ—çš„é•¿åº¦ã€‚
    _, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†ç”Ÿæˆé…ç½® (generation_config)ã€‚å¦‚æœæ²¡æœ‰æä¾›ï¼Œå°±ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤ç”Ÿæˆé…ç½®ã€‚
    if generation_config is None:
        generation_config = model.generation_config
    # ä½¿ç”¨ deepcopy æ¥å¤åˆ¶ç”Ÿæˆé…ç½®ã€‚è¿™ç¡®ä¿äº†åŸå§‹é…ç½®ä¸ä¼šåœ¨å‡½æ•°ä¸­è¢«ä¿®æ”¹ï¼Œä½¿å¾—è¿™ä¸ªå‡½æ•°å…·æœ‰çº¯åŠŸèƒ½æ€§è´¨ï¼ˆä¸æ”¹å˜å¤–éƒ¨çŠ¶æ€ï¼‰ã€‚
    generation_config = copy.deepcopy(generation_config)
    # æ›´æ–° generation_config å¯¹è±¡ï¼Œå°†ä»»ä½•é¢å¤–çš„å…³é”®å­—å‚æ•°ï¼ˆkwargsï¼‰åˆå¹¶åˆ°é…ç½®ä¸­ã€‚è¿™å…è®¸è°ƒç”¨è€…æ ¹æ®éœ€è¦è‡ªå®šä¹‰ç”Ÿæˆè¿‡ç¨‹ã€‚
    model_kwargs = generation_config.update(**kwargs)
    # æå–å¼€å§‹ç¬¦ (BOS) å’Œç»“æŸç¬¦ (EOS) çš„ token IDã€‚è¿™äº›tokenç”¨äºæ ‡è¯†ç”Ÿæˆæ–‡æœ¬çš„å¼€å§‹å’Œç»“æŸã€‚
    bos_token_id, eos_token_id = (
        generation_config.bos_token_id,
        generation_config.eos_token_id,
    )
    # æ£€æŸ¥eos_token_idæ˜¯å¦ä¸ºæ•´æ•°ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨ã€‚è¿™æ˜¯ä¸ºäº†å¤„ç†ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦çš„å¤šä¸ªç»“æŸç¬¦ã€‚
    if isinstance(eos_token_id, int):
        eos_token_id = [eos_token_id]
    # è¿™è¡Œä»£ç æ£€æŸ¥æ˜¯å¦æä¾›äº†é¢å¤–çš„ç»“æŸç¬¦ token ID (additional_eos_token_id)ã€‚
    # å¦‚æœæä¾›äº†ï¼Œå®ƒå°†è¢«æ·»åŠ åˆ° eos_token_id åˆ—è¡¨ä¸­ã€‚è¿™å…è®¸åŠ¨æ€æ‰©å±•æ–‡æœ¬ç”Ÿæˆçš„ç»“æŸæ¡ä»¶ã€‚
    if additional_eos_token_id is not None:
        eos_token_id.append(additional_eos_token_id)
    # æ£€æŸ¥æ˜¯å¦æ²¡æœ‰é€šè¿‡ kwargs æ˜¾å¼æä¾› max_length å‚æ•°ï¼Œå¹¶ä¸”åœ¨ generation_config ä¸­å·²ç»è®¾ç½®äº†é»˜è®¤çš„ max_lengthã€‚
    has_default_max_length = kwargs.get(
        'max_length') is None and generation_config.max_length is not None
    # å¦‚æœæ»¡è¶³ has_default_max_length ä¸” max_new_tokens æœªè®¾ç½®ï¼Œå°†å‘å‡ºè­¦å‘Šã€‚
    # è¿™è¯´æ˜ç”¨æˆ·ä¾èµ–äºè¿‡æ—¶çš„é…ç½®æ–¹æ³•æ¥é™åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œè¿™ç§åšæ³•åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­å°†ä¸å†æ”¯æŒã€‚
    if has_default_max_length and generation_config.max_new_tokens is None:
        warnings.warn(
            f"Using 'max_length''s default ({repr(generation_config.max_length)}) \
                to control the generation length. "
            'This behaviour is deprecated and will be removed from the \
                config in v5 of Transformers -- we'
            ' recommend using `max_new_tokens` to control the maximum \
                length of the generation.',
            UserWarning,
        )
    # å¦‚æœ max_new_tokens è¢«è®¾ç½®ï¼Œå®ƒå°†æ ¹æ®è¾“å…¥ ID çš„åºåˆ—é•¿åº¦è°ƒæ•´ max_length çš„å€¼ã€‚è¿™ä¿è¯ç”Ÿæˆçš„é•¿åº¦ä¸è¾“å…¥é•¿åº¦å’Œæ–°ç”Ÿæˆçš„ token æ•°é‡ç›¸é€‚åº”ã€‚
    elif generation_config.max_new_tokens is not None:
        generation_config.max_length = generation_config.max_new_tokens + \
                                       input_ids_seq_length
        if not has_default_max_length:
            # å¦‚æœ max_length å’Œ max_new_tokens éƒ½è¢«è®¾ç½®ï¼Œå°†å‘å‡ºè­¦å‘Šï¼Œå‘ŠçŸ¥ç”¨æˆ· max_new_tokens å°†ä¼˜å…ˆä½¿ç”¨ï¼Œå¹¶æ¨èæŸ¥é˜…ç›¸å…³æ–‡æ¡£äº†è§£æ›´å¤šä¿¡æ¯ã€‚
            logger.warning(  # pylint: disable=W4902
                f"Both 'max_new_tokens' (={generation_config.max_new_tokens}) "
                f"and 'max_length'(={generation_config.max_length}) seem to "
                "have been set. 'max_new_tokens' will take precedence. "
                'Please refer to the documentation for more information. '
                '(https://huggingface.co/docs/transformers/main/'
                'en/main_classes/text_generation)',
                UserWarning,
            )
    # æœ€åè¿™éƒ¨åˆ†æ£€æŸ¥è¾“å…¥çš„é•¿åº¦æ˜¯å¦è¶…è¿‡äº†è®¾ç½®çš„æœ€å¤§é•¿åº¦ (max_length)ã€‚
    # å¦‚æœæ˜¯è¿™æ ·ï¼Œå°†è®°å½•ä¸€æ¡è­¦å‘Šï¼ŒæŒ‡å‡ºè¿™å¯èƒ½å¯¼è‡´æ„å¤–çš„è¡Œä¸ºï¼Œå¹¶å»ºè®®å¢åŠ  max_new_tokens çš„å€¼ä»¥é¿å…è¿™ç§æƒ…å†µã€‚
    if input_ids_seq_length >= generation_config.max_length:
        input_ids_string = 'input_ids'
        logger.warning(
            f"Input length of {input_ids_string} is {input_ids_seq_length}, "
            f"but 'max_length' is set to {generation_config.max_length}. "
            'This can lead to unexpected behavior. You should consider'
            " increasing 'max_new_tokens'.")

    # 2. Set generation parameters if not already defined
    # ä¸‹é¢è¿™ä¸¤è¡Œç¡®ä¿ logits_processor å’Œ stopping_criteria éƒ½è¢«æ­£ç¡®åœ°åˆå§‹åŒ–ã€‚å¦‚æœå®ƒä»¬æ²¡æœ‰è¢«å¤–éƒ¨æä¾›ï¼ˆå³ä¸º Noneï¼‰ï¼Œ
    # åˆ™ä½¿ç”¨é»˜è®¤çš„ LogitsProcessorList å’Œ StoppingCriteriaList æ¥åˆå§‹åŒ–ã€‚è¿™äº›ç±»æ¥è‡ª transformers åº“ï¼Œæä¾›äº†åŸºç¡€çš„å¤„ç†å’Œåœæ­¢æœºåˆ¶ã€‚
    logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
    stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
    # è¿™ä¸€è¡Œè°ƒç”¨æ¨¡å‹çš„å†…éƒ¨æ–¹æ³• _get_logits_processor æ¥è·å–æˆ–é…ç½® logits å¤„ç†å™¨ã€‚
    # è¯¥æ–¹æ³•å¯èƒ½åŸºäºæä¾›çš„ generation_configã€è¾“å…¥çš„é•¿åº¦ã€è¾“å…¥ IDsã€ä»¥åŠä»»ä½•å‰ç¼€å…è®¸çš„ token å‡½æ•°æ¥è°ƒæ•´æˆ–å¢å¼ºä¼ å…¥çš„ logits_processorã€‚
    # è¿™æ˜¯ä¸ºäº†ç¡®ä¿ logits å¤„ç†å™¨èƒ½å¤Ÿé€‚åº”ç‰¹å®šçš„ç”Ÿæˆä»»åŠ¡å’Œé…ç½®ã€‚
    logits_processor = model._get_logits_processor(
        generation_config=generation_config,
        input_ids_seq_length=input_ids_seq_length,
        encoder_input_ids=input_ids,
        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
        logits_processor=logits_processor,
    )
    # ç±»ä¼¼åœ°ï¼Œè¿™ä¸€è¡Œè°ƒç”¨æ¨¡å‹çš„ _get_stopping_criteria æ–¹æ³•æ¥é…ç½®æˆ–è·å–åœæ­¢ç”Ÿæˆçš„æ¡ä»¶ã€‚
    # è¿™å¯ä»¥æ ¹æ® generation_config å’Œå·²æœ‰çš„ stopping_criteria è¿›è¡Œè°ƒæ•´ï¼Œç¡®ä¿ç”Ÿæˆè¿‡ç¨‹èƒ½åœ¨é€‚å½“çš„æ—¶æœºåœæ­¢ã€‚
    stopping_criteria = model._get_stopping_criteria(
        generation_config=generation_config,
        stopping_criteria=stopping_criteria)
    # è¿™ä¸€è¡Œè·å–ä¸€ä¸ª logits_warperï¼Œå®ƒæ˜¯ç”¨äºè°ƒæ•´ logits ä»¥æ”¹å˜ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒçš„å·¥å…·ã€‚è¿™é€šå¸¸ç”¨äºå®ç°å¦‚æ¸©åº¦è°ƒèŠ‚æˆ– top-k sampling ç­‰é«˜çº§ç”ŸæˆæŠ€å·§ã€‚
    logits_warper = model._get_logits_warper(generation_config)
    # åˆå§‹åŒ–ä¸€ä¸ªä¸ input_ids ç›¸åŒå¤§å°çš„ tensorï¼Œç”¨äºè·Ÿè¸ªå“ªäº›åºåˆ—å°šæœªå®Œæˆã€‚æ‰€æœ‰å…ƒç´ åˆå§‹è®¾ç½®ä¸º 1ï¼ˆè¡¨ç¤ºåºåˆ—ä»åœ¨ç”Ÿæˆä¸­ï¼‰ã€‚
    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)
    # åˆå§‹åŒ– scores ä¸º Noneï¼Œå¯èƒ½ç¨åç”¨äºå­˜å‚¨è°ƒè¯•æˆ–åˆ†æç›®çš„çš„åˆ†æ•°æˆ–æ¦‚ç‡ã€‚
    scores = None
    # å¼€å§‹ä¸€ä¸ªæ— é™å¾ªç¯ï¼Œç›´åˆ°é‡åˆ°ä¸­æ–­æ¡ä»¶æ‰åœæ­¢ï¼Œé€šå¸¸æ˜¯å½“æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹å®Œæˆæˆ–è§¦å‘åœæ­¢æ ‡å‡†æ—¶ã€‚
    while True:
        # æ ¹æ®å½“å‰ input_ids çš„çŠ¶æ€å’Œé¢å¤–çš„å‚æ•°ï¼ˆmodel_kwargsï¼‰å‡†å¤‡æ¨¡å‹çš„è¾“å…¥æ•°æ®ã€‚è¿™ä¸€æ­¥éª¤é€šå¸¸å°†è¾“å…¥æ ¼å¼åŒ–ä¸ºç¬¦åˆæ¨¡å‹é¢„æœŸè¾“å…¥ç»“æ„çš„æ–¹å¼ã€‚
        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)
        # ä½¿ç”¨å‡†å¤‡å¥½çš„è¾“å…¥é€šè¿‡æ¨¡å‹è¿›è¡Œå‰å‘ä¼ é€’ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªå¯èƒ½æ ‡è®°çš„ logitsã€‚return_dict=True æŒ‡å®šè¾“å‡ºåº”è¯¥ä»¥å­—å…¸å½¢å¼è¿”å›ã€‚
        # output_attentions å’Œ output_hidden_states è®¾ç½®ä¸º Falseï¼Œä»¥æœ€å°åŒ–å†…å­˜ä½¿ç”¨ï¼Œé™¤ééœ€è¦è¿™äº›å€¼ã€‚
        outputs = model(
            **model_inputs,
            return_dict=True,
            output_attentions=False,
            output_hidden_states=False,
        )
        # ä»è¾“å‡ºä¸­æå–æœ€åä¸€ä¸ªæ ‡è®°ä½ç½®çš„ logitsï¼Œè¿™åŒ…å«äº†æ¨¡å‹å¯¹ä¸‹ä¸€ä¸ªæ ‡è®°çš„é¢„æµ‹ã€‚
        next_token_logits = outputs.logits[:, -1, :]

        # pre-process distribution
        # ä½¿ç”¨ logits_processor å¤„ç† logitsï¼ˆå¯ä»¥å®ç°é˜²æ­¢æ ‡è®°é‡å¤çš„æœºåˆ¶ï¼‰ï¼Œ
        # ç„¶åä½¿ç”¨ logits_warperï¼ˆå¯èƒ½å®ç°å¦‚æ¸©åº¦è°ƒèŠ‚æˆ– top-k é‡‡æ ·ç­‰é«˜çº§ç”Ÿæˆç­–ç•¥ï¼‰ã€‚
        next_token_scores = logits_processor(input_ids, next_token_logits)
        next_token_scores = logits_warper(input_ids, next_token_scores)

        # sample
        # ä½¿ç”¨ softmax å‡½æ•°å°†å¤„ç†åçš„ logits è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œsoftmax å‡½æ•°å°† logits æ ‡å‡†åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚
        probs = nn.functional.softmax(next_token_scores, dim=-1)
        # é€šè¿‡ä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ï¼ˆå¦‚æœ do_sample ä¸º Trueï¼‰æˆ–é€‰æ‹©æœ€é«˜æ¦‚ç‡çš„æ ‡è®°æ¥å†³å®šä¸‹ä¸€ä¸ªæ ‡è®°ï¼ˆTokenï¼‰ã€‚
        if generation_config.do_sample:
            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
        else:
            next_tokens = torch.argmax(probs, dim=-1)

        # update generated ids, model inputs, and length for next step
        # å°†æ–°é€‰æ‹©çš„æ ‡è®°è¿æ¥åˆ°ä¸‹ä¸€æ¬¡è¿­ä»£çš„ input_ids åºåˆ—ä¸­ã€‚
        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
        # æ ¹æ®å½“å‰çš„è¾“å‡ºæ›´æ–°ä¸‹ä¸€æ¬¡è¿­ä»£çš„å…³é”®å­—å‚æ•°ï¼Œè¿™å¯èƒ½åŒ…æ‹¬æ›´æ–°æ³¨æ„åŠ›æ©ç æˆ–å…¶ä»–ä¸‹ä¸€æ¬¡å‰å‘ä¼ é€’æ‰€éœ€çš„çŠ¶æ€ã€‚
        model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=False)
        # æ›´æ–° unfinished_sequences ä»¥è·Ÿè¸ªå“ªäº›åºåˆ—ä»åœ¨ç”Ÿæˆä¸­ï¼Œå“ªäº›å·²å®Œæˆï¼ŒåŸºäºä¸‹ä¸€ä¸ªæ ‡è®°æ˜¯å¦ä¸ä»»ä½• EOSï¼ˆå¥æœ«æ ‡è®°ï¼‰æ ‡è®°åŒ¹é…ã€‚
        unfinished_sequences = unfinished_sequences.mul((min(next_tokens != i for i in eos_token_id)).long())
        # æå–å¹¶è§£ç ç”Ÿæˆçš„æ ‡è®°ä¸ºäººç±»å¯è¯»çš„æ–‡æœ¬ï¼ŒåŒæ—¶å¤„ç†å¦‚æœæœ€åä¸€ä¸ªæ ‡è®°æ˜¯ EOS æ ‡è®°çš„å¯èƒ½æ€§ã€‚
        output_token_ids = input_ids[0].cpu().tolist()
        output_token_ids = output_token_ids[input_length:]
        for each_eos_token_id in eos_token_id:
            if output_token_ids[-1] == each_eos_token_id:
                output_token_ids = output_token_ids[:-1]
        response = tokenizer.decode(output_token_ids, skip_special_tokens=True)
        # å°†ç”Ÿæˆçš„å“åº”è¿”å›ç»™è°ƒç”¨è€…ï¼Œå…è®¸å‡½æ•°äº§ç”Ÿè¾“å‡ºæµè€Œä¸æ˜¯å•æ‰¹è¿”å›ã€‚
        yield response
        # stop when each sentence is finished
        # or if we exceed the maximum length
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åºåˆ—éƒ½å·²å®Œæˆï¼ˆå³åœ¨ unfinished_sequences ä¸­å…¨éƒ¨æ ‡è®°ä¸ºå®Œæˆï¼‰æˆ–æ˜¯å¦æ»¡è¶³ä»»ä½•å¤–éƒ¨åœæ­¢æ ‡å‡†ï¼Œå¦‚æœæ˜¯è¿™æ ·ï¼Œå°±ä¸­æ–­å¾ªç¯ã€‚
        if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):
            break


def on_btn_click():
    """
    è¿™ä¸ªå‡½æ•°å®šä¹‰äº†ä¸€ä¸ªäº‹ä»¶å¤„ç†å™¨ï¼Œå½“ç›¸å…³çš„æŒ‰é’®è¢«ç‚¹å‡»æ—¶è§¦å‘ã€‚è¿™é‡Œçš„æ“ä½œæ˜¯åˆ é™¤ Streamlit ä¼šè¯çŠ¶æ€ä¸­å­˜å‚¨çš„ messagesã€‚
    è¯¥åŠŸèƒ½é€šå¸¸ç”¨äºé‡ç½®åº”ç”¨çš„çŠ¶æ€æˆ–æ¸…é™¤ç¼“å­˜çš„æ•°æ®ã€‚
    """
    del st.session_state.messages


@st.cache_resource  # è¿™ä¸ªå‡½æ•°ä½¿ç”¨ @st.cache_resource è£…é¥°å™¨ï¼Œä½¿å¾— Streamlit å¯ä»¥ç¼“å­˜è¯¥å‡½æ•°çš„ç»“æœã€‚
def load_model(model_name_or_path, adapter_name_or_path=None, load_in_4bit=False):
    """
    :param model_name_or_path: æŒ‡å®šæ¨¡å‹çš„åç§°æˆ–è·¯å¾„
    :param adapter_name_or_path: å¯é€‰ï¼ŒæŒ‡å®šé€‚é…å™¨çš„åç§°æˆ–è·¯å¾„
    :param load_in_4bit: æ˜¯å¦åœ¨ 4 ä½é‡åŒ–æ¨¡å¼ä¸‹åŠ è½½æ¨¡å‹
    :return: è¿”å›åŠ è½½çš„æ¨¡å‹å’Œåˆ†è¯å™¨
    """
    # è¿™æ®µæ¡ä»¶ä»£ç æ£€æŸ¥æ˜¯å¦éœ€è¦ä»¥ 4 ä½é‡åŒ–æ¨¡å¼åŠ è½½æ¨¡å‹ã€‚
    # å¦‚æœæ˜¯ï¼Œå®ƒå°†åˆ›å»ºä¸€ä¸ªé‡åŒ–é…ç½®å¯¹è±¡ BitsAndBytesConfigï¼Œé…ç½®äº†å„ç§é‡åŒ–å‚æ•°ï¼Œå¦‚è®¡ç®—æ•°æ®ç±»å‹å’Œé‡åŒ–ç±»å‹ã€‚
    # å¦‚æœä¸åŠ è½½ 4 ä½é‡åŒ–æ¨¡å¼ï¼Œåˆ™é‡åŒ–é…ç½®è®¾ç½®ä¸º Noneã€‚
    if load_in_4bit:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,  # é‡åŒ–è¿‡ç¨‹ä¸­ä½¿ç”¨çš„è®¡ç®—æ•°æ®ç±»å‹ã€‚
            bnb_4bit_use_double_quant=True,  # è¿™ä¸ªå‚æ•°å¯ç”¨åŒé‡é‡åŒ–ç­–ç•¥ã€‚åœ¨é™ä½ä½å®½çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿ç•™æ›´å¤šçš„ä¿¡æ¯ã€‚
            bnb_4bit_quant_type="nf4",
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,  # æŒ‡ç¤ºæƒé‡æ˜¯å¦ä»¥ FP16 æ ¼å¼å­˜å‚¨ã€‚
        )
    else:
        quantization_config = None
    # è¿™æ®µä»£ç ä½¿ç”¨ transformers åº“ä¸­çš„ AutoModelForCausalLM.from_pretrainedï¼ŒåŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„å› æœè¯­è¨€æ¨¡å‹causal language modelï¼‰ã€‚
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,  # æŒ‡å®šæ¨¡å‹çš„åç§°æˆ–æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„ã€‚
        load_in_4bit=load_in_4bit,  # æŒ‡å®šæ˜¯å¦åœ¨ 4 ä½é‡åŒ–æ¨¡å¼ä¸‹åŠ è½½æ¨¡å‹, å¯ä»¥æ˜¾è‘—å‡å°‘æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚
        trust_remote_code=True,  # åœ¨åŠ è½½è¿œç¨‹æˆ–è‡ªå®šä¹‰æ¨¡å‹æ—¶ï¼Œæ˜¯å¦ä¿¡ä»»å’Œæ‰§è¡Œæ¨¡å‹æ–‡ä»¶ä¸­åŒ…å«çš„è‡ªå®šä¹‰ä»£ç ã€‚
        low_cpu_mem_usage=False,  # è¿™ä¸ªé€‰é¡¹åœ¨åŠ è½½æ¨¡å‹æ—¶å‡å°‘ CPU å†…å­˜çš„ä½¿ç”¨ã€‚é€‚ç”¨äºå†…å­˜èµ„æºå—é™çš„ç¯å¢ƒã€‚
        torch_dtype=torch.float16,  # æŒ‡å®šæ¨¡å‹ä½¿ç”¨çš„ PyTorch æ•°æ®ç±»å‹ã€‚
        device_map='auto',  # æŒ‡å®šæ¨¡å‹åº”è¯¥åŠ è½½åˆ°å“ªä¸ªè®¾å¤‡ä¸Šã€‚'auto' è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„è®¾å¤‡ã€‚
        quantization_config=quantization_config  # æä¾›é‡åŒ–é…ç½®ï¼Œè¿™æ˜¯é€šè¿‡ BitsAndBytesConfig æˆ–å…¶ä»–ç›¸å…³é…ç½®ç±»è®¾ç½®çš„ã€‚
    )
    # å¦‚æœæä¾›äº†é€‚é…å™¨è·¯å¾„æˆ–åç§°ï¼Œåˆ™åŠ è½½ä¸€ä¸ª PeftModelï¼Œè¿™æ˜¯ä¸€ç§æ”¯æŒé€‚é…å™¨çš„æ¨¡å‹ï¼Œå¯ä»¥ç”¨æ¥å¢å¼ºæˆ–è°ƒæ•´åŸæœ‰æ¨¡å‹çš„è¡Œä¸ºã€‚
    if adapter_name_or_path is not None:
        model = PeftModel.from_pretrained(model, adapter_name_or_path)
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œè¿™é€šå¸¸åœ¨è¿›è¡Œé¢„æµ‹æˆ–æ¨ç†æ—¶éœ€è¦ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æŸäº›è®­ç»ƒç‰¹å®šè¡Œä¸ºï¼ˆå¦‚ Dropoutï¼‰è¢«ç¦ç”¨ã€‚
    model.eval()
    # åŠ è½½ä¸æ¨¡å‹ç›¸å¯¹åº”çš„åˆ†è¯å™¨ (AutoTokenizer)ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)

    return model, tokenizer


def prepare_generation_config():
    """
    è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªå‡½æ•° prepare_generation_config()ï¼Œç”¨äºåœ¨ Streamlit åº”ç”¨ä¸­é…ç½®å’Œæ˜¾ç¤ºæ–‡æœ¬ç”Ÿæˆç›¸å…³çš„è¶…å‚æ•°ã€‚
    è¯¥å‡½æ•°åˆ©ç”¨ Streamlit çš„ UI ç»„ä»¶åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªäº¤äº’å¼çš„æ§åˆ¶é¢æ¿ã€‚
    :return: è¿”å›è¿™ä¸ªé…ç½®å¯¹è±¡ï¼Œä½¿å…¶å¯ä»¥åœ¨å…¶ä»–éƒ¨åˆ†çš„åº”ç”¨ä¸­ç”¨äºæ§åˆ¶æ–‡æœ¬ç”Ÿæˆè¡Œä¸ºã€‚
    """
    with st.sidebar:  # è¯­å¥æŒ‡å®šæ¥ä¸‹æ¥çš„ Streamlit ç»„ä»¶å°†æ˜¾ç¤ºåœ¨åº”ç”¨çš„ä¾§è¾¹æ ä¸­ã€‚
        # TODO ä¿®æ”¹è¿™éƒ¨åˆ†é€»è¾‘ï¼ŒåŠ å…¥pdfæ–‡ä»¶çš„ä¸Šä¼ ã€ä¸‹è½½ç­‰ï¼Œå¹¶ç¼–å†™å¯¹åº”çš„é€»è¾‘
        st.title('è¶…å‚æ•°é¢æ¿')
        # st.text_area åˆ›å»ºäº†ä¸€ä¸ªæ–‡æœ¬è¾“å…¥åŒºåŸŸï¼Œç”¨æˆ·å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥æˆ–ä¿®æ”¹é¢„è®¾çš„æç¤ºæ–‡æœ¬ã€‚
        system_prompt_content = st.text_area('ç³»ç»Ÿæç¤ºè¯',
                                             "ä½ æ˜¯ä¸€ä¸ªæœ‰åˆ›é€ çš„è¶…çº§äººå·¥æ™ºèƒ½assistant,åå­—å«Llama3-Chinese,æ‹¥æœ‰å…¨äººç±»çš„æ‰€æœ‰çŸ¥è¯†ã€‚"
                                             "ä½ å–œæ¬¢ç”¨å¹½é»˜é£è¶£çš„è¯­è¨€å›å¤ç”¨æˆ·ï¼Œä½†ä½ æ›´å–œæ¬¢ç”¨å‡†ç¡®ã€æ·±å…¥çš„ç­”æ¡ˆã€‚"
                                             "ä½ éœ€è¦ç»“åˆä¸­å›½çš„æ–‡åŒ–å’ŒèŠå¤©è®°å½•ä¸­çš„ä¸Šæ–‡è¯é¢˜ç†è§£å’Œæ¨æµ‹ç”¨æˆ·çœŸæ­£æ„å›¾ï¼ŒæŒ‰è¦æ±‚æ­£ç¡®å›å¤ç”¨æˆ·é—®é¢˜ã€‚"
                                             "æ³¨æ„ä½¿ç”¨æ°å½“çš„æ–‡ä½“å’Œæ ¼å¼è¿›è¡Œå›å¤ï¼Œå°½é‡é¿å…é‡å¤æ–‡å­—å’Œé‡å¤å¥å­ï¼Œä¸”å•æ¬¡å›å¤å°½å¯èƒ½ç®€æ´æ·±é‚ƒã€‚"
                                             "ä½ å…³æ³¨è®¨è®ºçš„ä¸Šä¸‹æ–‡ï¼Œæ·±æ€ç†Ÿè™‘åœ°å›å¤ç”¨æˆ·"
                                             "å¦‚æœä½ ä¸çŸ¥é“æŸä¸ªé—®é¢˜çš„å«ä¹‰ï¼Œè¯·è¯¢é—®ç”¨æˆ·ï¼Œå¹¶å¼•å¯¼ç”¨æˆ·è¿›è¡Œæé—®ã€‚"
                                             "å½“ç”¨æˆ·è¯´ç»§ç»­æ—¶,è¯·æ¥ç€aissistantä¸Šä¸€æ¬¡çš„å›ç­”è¿›è¡Œç»§ç»­å›å¤ã€‚",
                                             height=200,  # height=200 è®¾ç½®äº†æ–‡æœ¬åŒºåŸŸçš„é«˜åº¦ã€‚
                                             # key='system_prompt_content' ä¸ºè¿™ä¸ª UI ç»„ä»¶å®šä¹‰äº†ä¸€ä¸ªå”¯ä¸€çš„é”®å€¼ï¼Œå¯ä»¥ç”¨äºåç»­æ“ä½œã€‚
                                             key='system_prompt_content'
                                             )  # TODO USELESS
        # è¶…å‚æ•°æ»‘å—ï¼Œst.slider ç»„ä»¶å…è®¸ç”¨æˆ·äº¤äº’å¼åœ°é€‰æ‹©æ–‡æœ¬ç”Ÿæˆçš„å„ç§å‚æ•°
        max_new_tokens = st.slider('æœ€å¤§å›å¤é•¿åº¦', 100, 8192, 660, step=8)  # æ§åˆ¶ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ã€‚
        top_p = st.slider('Top P', 0.0, 1.0, 0.8, step=0.01)  # è®¾ç½®é‡‡æ ·çš„ softmax æ¦‚ç‡é˜ˆå€¼ï¼Œç”¨äºæ§åˆ¶æ–‡æœ¬å¤šæ ·æ€§ã€‚
        temperature = st.slider('æ¸©åº¦ç³»æ•°', 0.0, 1.0, 0.7, step=0.01)  # è°ƒèŠ‚éšæœºæ€§çš„å¤§å°ï¼Œå½±å“ç”Ÿæˆæ–‡æœ¬çš„ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ã€‚
        repetition_penalty = st.slider("é‡å¤æƒ©ç½šç³»æ•°", 1.0, 2.0, 1.07, step=0.01)  # ç”¨äºé™ä½é‡å¤å†…å®¹çš„å‘ç”Ÿã€‚
        st.button('é‡ç½®èŠå¤©', on_click=on_btn_click)  # åˆ›å»ºä¸€ä¸ªæŒ‰é’®ï¼Œå½“è¢«ç‚¹å‡»æ—¶è§¦å‘ on_btn_click å‡½æ•°ï¼Œè¯¥å‡½æ•°å¯ä»¥ç”¨æ¥é‡ç½®èŠå¤©çŠ¶æ€æˆ–æ¸…é™¤ä¼šè¯æ•°æ®ã€‚

    generation_config = GenerationConfig(max_new_tokens=max_new_tokens,
                                         top_p=top_p,
                                         temperature=temperature,
                                         repetition_penalty=repetition_penalty,
                                         )
    return generation_config


system_prompt = '<|begin_of_text|><<SYS>>\n{content}\n<</SYS>>\n\n'
user_prompt = '<|start_header_id|>user<|end_header_id|>\n\n{user}<|eot_id|>'
robot_prompt = '<|start_header_id|>assistant<|end_header_id|>\n\n{robot}<|eot_id|>'
cur_query_prompt = '<|start_header_id|>user<|end_header_id|>\n\n{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n'
"""
:system_prompt: ç³»ç»Ÿæç¤ºæ¨¡æ¿ã€‚å®ƒç”¨æ¥å°è£…ä¸€äº›ç³»ç»Ÿçº§çš„ä¿¡æ¯æˆ–æŒ‡ä»¤ï¼Œå¯èƒ½ç”¨äºæ§åˆ¶æˆ–æŒ‡å¯¼ç”Ÿæˆæ¨¡å‹çš„è¡Œä¸ºã€‚
    ä½¿ç”¨ <<SYS>> å’Œ <</SYS>> æ ‡ç­¾æ¥æ˜ç¡®åœ°æ ‡è¯†åŒ…å›´çš„å†…å®¹æ˜¯ç³»ç»Ÿçº§çš„ã€‚
    {content} æ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œç”¨äºæ’å…¥å®é™…çš„ç³»ç»Ÿä¿¡æ¯æˆ–æŒ‡ä»¤ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡æ¿å¯ä»¥çµæ´»åœ°ç”¨äºä¸åŒçš„ç³»ç»Ÿä¿¡æ¯ã€‚
    æ¨¡æ¿åŒ…å«æ¢è¡Œç¬¦ (\n) ä»¥ä¿æŒæ¸…æ™°çš„æ ¼å¼åŒ–è¾“å‡ºã€‚
:user_prompt: ç”¨æˆ·è¾“å…¥çš„æ ¼å¼æ¨¡æ¿ã€‚å®ƒæ ‡è®°äº†ä¸€ä¸ªç”¨æˆ·å‘è¨€çš„å¼€å§‹ã€‚
:robot_prompt: ç±»ä¼¼äº user_promptï¼ŒåŠ©æ‰‹æˆ–æœºå™¨äººå›åº”çš„æ ¼å¼æ¨¡æ¿ã€‚
:cur_query_prompt: è¿™æ˜¯ä¸€ä¸ªç”¨äºå½“å‰æŸ¥è¯¢çš„å¤åˆæç¤ºæ¨¡æ¿ï¼Œå®ƒç»“åˆäº†ç”¨æˆ·çš„å‘è¨€å’ŒåŠ©æ‰‹çš„å“åº”
"""


def combine_history(prompt):
    """
    æ•´åˆèŠå¤©å†å²è®°å½•å¹¶æ„é€ ç”¨äºæ–‡æœ¬ç”Ÿæˆç³»ç»Ÿçš„å®Œæ•´è¾“å…¥ã€‚
    è¯¥å‡½æ•°é‡‡ç”¨å½“å‰çš„ç”¨æˆ·è¾“å…¥ï¼ˆpromptï¼‰å’Œä¼šè¯å†å²ï¼Œç”Ÿæˆä¸€ä¸ªæ ¼å¼åŒ–çš„æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œè¯¥å­—ç¬¦ä¸²åŒ…å«äº†æ‰€æœ‰å…ˆå‰çš„å¯¹è¯ä»¥åŠå½“å‰çš„æŸ¥è¯¢ã€‚
    :param prompt: å½“å‰çš„ç”¨æˆ·è¾“å…¥
    :return: æ‰€æœ‰å…ˆå‰çš„å¯¹è¯ä»¥åŠå½“å‰çš„æŸ¥è¯¢
    """
    # ä» Streamlit çš„ä¼šè¯çŠ¶æ€ä¸­è·å– messages åˆ—è¡¨ï¼Œè¿™é‡Œå‡è®¾ messages æ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«èŠå¤©æ¶ˆæ¯çš„å†…å®¹å’Œè§’è‰²ï¼ˆç”¨æˆ·æˆ–æœºå™¨äººï¼‰ã€‚
    messages = st.session_state.messages
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—ç¬¦ä¸² total_promptï¼Œç”¨äºç´¯ç§¯æ•´ä¸ªå¯¹è¯çš„å†…å®¹ã€‚
    total_prompt = ''
    # éå† messages åˆ—è¡¨ä¸­çš„æ¯ä¸ªæ¶ˆæ¯ã€‚
    for message in messages:
        # æå–æ¯ä¸ªæ¶ˆæ¯çš„å†…å®¹ (cur_content) å’Œè§’è‰²ï¼Œå¹¶æ ¹æ®è§’è‰²ä½¿ç”¨é€‚å½“çš„æ ¼å¼æ¨¡æ¿ï¼ˆä¹‹å‰å®šä¹‰çš„ user_prompt æˆ– robot_promptï¼‰æ¥æ ¼å¼åŒ–æ¶ˆæ¯ã€‚
        cur_content = message['content']
        if message['role'] == 'user':
            cur_prompt = user_prompt.format(user=cur_content)
        elif message['role'] == 'robot':
            cur_prompt = robot_prompt.format(robot=cur_content)
        else:
            raise RuntimeError
        total_prompt += cur_prompt
    # ä» Streamlit çš„ä¼šè¯çŠ¶æ€ä¸­è·å– system_prompt_contentï¼Œè¿™å¯èƒ½æ˜¯é¢„å®šä¹‰çš„æˆ–è€…æ˜¯ç”¨æˆ·åœ¨æŸä¸ªç•Œé¢å…ƒç´ ä¸­è¾“å…¥çš„ç³»ç»Ÿçº§æç¤ºã€‚
    system_prompt_content = st.session_state.system_prompt_content
    # ä½¿ç”¨ system_prompt æ¨¡æ¿æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºã€‚
    system = system_prompt.format(content=system_prompt_content)
    # å°†ç³»ç»Ÿæç¤ºã€ç´¯ç§¯çš„å¯¹è¯å†…å®¹ä»¥åŠä½¿ç”¨ cur_query_prompt æ¨¡æ¿æ ¼å¼åŒ–çš„å½“å‰ç”¨æˆ·è¾“å…¥æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆæœ€ç»ˆçš„ total_promptã€‚
    total_prompt = system + total_prompt + cur_query_prompt.format(user=prompt)

    return total_prompt


def main(model_name_or_path, adapter_name_or_path):
    print(f'{Color.B}[Academic Code Annotator]{Color.RE}Loading model...')
    # è°ƒç”¨ load_model å‡½æ•°åŠ è½½æŒ‡å®šçš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚è¿™é‡Œï¼Œload_in_4bit=False è¡¨ç¤ºä¸ä½¿ç”¨ 4 ä½é‡åŒ–åŠ è½½æ¨¡å‹ã€‚
    model, tokenizer = load_model(model_name_or_path, adapter_name_or_path=adapter_name_or_path, load_in_4bit=True)
    print(f'{Color.B}[Academic Code Annotator]{Color.RE}{Color.G}Load model successful!{Color.RE}')

    # è®¾ç½® Streamlit é¡µé¢æ ‡é¢˜
    st.title('Llama3-Chinese')  # TODO

    # è°ƒç”¨ prepare_generation_config å‡½æ•°æ¥è®¾ç½®å¹¶è·å–æ–‡æœ¬ç”Ÿæˆçš„é…ç½®å‚æ•°ã€‚
    generation_config = prepare_generation_config()  # TODO æ»‘å—è²Œä¼¼åªåœ¨è¿™é‡Œåšäº†å˜åŒ–ï¼Œæ²¡æœ‰åšåˆ°åŠ¨æ€æ›´æ–°

    # åˆå§‹åŒ–èŠå¤©å†å²
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²èŠå¤©æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message['role']):
            st.markdown(message['content'])

    # Accept user input
    if prompt := st.chat_input('è§£é‡Šä¸€ä¸‹Vueçš„åŸç†'):  # ä½¿ç”¨ st.chat_input è·å–ç”¨æˆ·çš„è¾“å…¥ã€‚
        # ä½¿ç”¨ st.chat_message æ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥ã€‚
        with st.chat_message('user'):
            st.markdown(prompt)
        real_prompt = combine_history(prompt)  # è°ƒç”¨ combine_history å‡½æ•°å°†å½“å‰è¾“å…¥ä¸å†å²æ¶ˆæ¯ç»„åˆï¼Œå‡†å¤‡å‘é€ç»™æ¨¡å‹ã€‚
        # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°ä¼šè¯çŠ¶æ€çš„ messages åˆ—è¡¨ã€‚
        st.session_state.messages.append({
            'role': 'user',
            'content': prompt,
        })

        # ä½¿ç”¨ st.chat_message æ¥ä¸ºæœºå™¨äººçš„å›å¤åˆ›å»ºä¸€ä¸ªæ¶ˆæ¯å®¹å™¨ã€‚
        with st.chat_message('robot'):
            message_placeholder = st.empty()
            # ä½¿ç”¨ generate_interactive å‡½æ•°ç”Ÿæˆå›å¤ï¼ŒæœŸé—´é€šè¿‡ message_placeholder å®æ—¶æ›´æ–°æ˜¾ç¤ºçš„å†…å®¹ã€‚
            for cur_response in generate_interactive(
                    model=model,
                    tokenizer=tokenizer,
                    prompt=real_prompt,
                    additional_eos_token_id=128009,
                    **asdict(generation_config),
            ):
                # åœ¨èŠå¤©æ¶ˆæ¯å®¹å™¨ä¸­æ˜¾ç¤ºæœºå™¨äººå“åº”
                message_placeholder.markdown(cur_response + 'â–Œ')
            message_placeholder.markdown(cur_response)
        # å®Œæˆç”Ÿæˆåï¼Œå°†æœºå™¨äººçš„æœ€ç»ˆå›å¤æ·»åŠ åˆ°ä¼šè¯çŠ¶æ€ã€‚
        st.session_state.messages.append({
            'role': 'robot',
            'content': cur_response,  # pylint: disable=undefined-loop-variable
        })
        # åœ¨ç”Ÿæˆè¿‡ç¨‹ç»“æŸåæ¸…ç©º CUDA ç¼“å­˜ï¼Œä»¥ç®¡ç† GPU å†…å­˜ä½¿ç”¨ã€‚
        torch.cuda.empty_cache()


if __name__ == '__main__':
    # å¯¼å…¥ Python çš„ç³»ç»Ÿæ¨¡å— sysï¼Œå®ƒåŒ…å«äº†ä¸ Python è§£é‡Šå™¨å’Œå®ƒçš„ç¯å¢ƒæœ‰å…³çš„åŠŸèƒ½ï¼Œæ¯”å¦‚å‘½ä»¤è¡Œå‚æ•°ã€‚
    import sys
    # sys.argv æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†å‘½ä»¤è¡Œå‚æ•°ã€‚sys.argv[0] æ˜¯è„šæœ¬åï¼Œsys.argv[1] é€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªå‚æ•°ï¼Œè¿™é‡Œè¢«ç”¨æ¥æŒ‡å®šæ¨¡å‹çš„åç§°æˆ–è·¯å¾„ã€‚
    model_name_or_path = sys.argv[1]
    # è¿™é‡Œæ£€æŸ¥ sys.argv çš„é•¿åº¦æ˜¯å¦å¤§äºç­‰äº 3ï¼Œä»¥ç¡®å®šæ˜¯å¦æœ‰ç¬¬äºŒä¸ªå‘½ä»¤è¡Œå‚æ•°æä¾›ï¼ˆå³ sys.argv[2]ï¼‰ã€‚å¦‚æœæœ‰ï¼Œå°†å…¶ä½œä¸ºé€‚é…å™¨çš„åç§°æˆ–è·¯å¾„ã€‚
    if len(sys.argv) >= 3:
        adapter_name_or_path = sys.argv[2]
    else:
        adapter_name_or_path = None
    # è°ƒç”¨ä¸»å‡½æ•°
    main(model_name_or_path, adapter_name_or_path)
